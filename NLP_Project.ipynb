{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priya1Projects/MSBA_DataScience/blob/main/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib-iXQf50gGO",
        "outputId": "dfaf81df-91b3-471f-e6b0-7cc8b8957915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.2.0-py3-none-any.whl (983 kB)\n",
            "\u001b[K     |████████████████████████████████| 983 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 35.6 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n",
            "\u001b[K     |████████████████████████████████| 359 kB 18.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.5.18.1)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 36.9 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.2.0)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.2 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.2.0 sniffio-1.2.0 trio-0.20.0 trio-websocket-0.9.2 urllib3-1.26.9 wsproto-1.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting maskpass\n",
            "  Downloading maskpass-0.3.6-py3-none-any.whl (9.1 kB)\n",
            "Collecting pynput\n",
            "  Downloading pynput-1.7.6-py2.py3-none-any.whl (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting python-xlib>=0.17\n",
            "  Downloading python_xlib-0.31-py2.py3-none-any.whl (179 kB)\n",
            "\u001b[K     |████████████████████████████████| 179 kB 39.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pynput->maskpass) (1.15.0)\n",
            "Collecting evdev>=1.3\n",
            "  Downloading evdev-1.5.0.tar.gz (26 kB)\n",
            "Building wheels for collected packages: evdev\n",
            "  Building wheel for evdev (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for evdev: filename=evdev-1.5.0-cp37-cp37m-linux_x86_64.whl size=97538 sha256=4d1b35338cc45650f16dc892965e568c5eccaede8afd071e9cb7e582f22cb957\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/22/83/8cce4c52d67fca4bf2eb7a1de1ecd9053b992d9b95efae3fad\n",
            "Successfully built evdev\n",
            "Installing collected packages: python-xlib, evdev, pynput, maskpass\n",
            "Successfully installed evdev-1.5.0 maskpass-0.3.6 pynput-1.7.6 python-xlib-0.31\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium \n",
        "!pip install beautifulsoup4\n",
        "!pip install maskpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Txn9nXO40gGT"
      },
      "outputs": [],
      "source": [
        "\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import sys\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from time import sleep\n",
        "import getpass\n",
        "import urllib.request"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install chromium, its driver, and selenium\n",
        "# !apt-get update\n",
        "# !apt install chromium-chromedriver\n",
        "# !cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "# !pip install selenium\n",
        "# set options to be headless, ..\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "# open it, go to a website, and get results\n",
        "wd = webdriver.Chrome('chromedriver',options=options)"
      ],
      "metadata": {
        "id": "QSo87YFNCqmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6w74k_OF0gGU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "driver = webdriver.Chrome('chromedriver',options=options)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPJL_MgpWBBt",
        "outputId": "5a37b2e8-9825-4794-accf-5d1e3470522b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: pandas\n",
            "Version: 1.3.5\n",
            "Summary: Powerful data structures for data analysis, time series, and statistics\n",
            "Home-page: https://pandas.pydata.org\n",
            "Author: The Pandas Development Team\n",
            "Author-email: pandas-dev@python.org\n",
            "License: BSD-3-Clause\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: numpy, pytz, python-dateutil\n",
            "Required-by: xarray, vega-datasets, statsmodels, sklearn-pandas, seaborn, pymc3, plotnine, pandas-profiling, pandas-gbq, pandas-datareader, mlxtend, mizani, holoviews, gspread-dataframe, google-colab, fix-yahoo-finance, fbprophet, fastai, cufflinks, cmdstanpy, arviz, altair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mgi1PHUW1ATs",
        "outputId": "47b2ed9d-4095-43b8-c6cb-d021433e1c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfxtfxba0gGW"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "import time\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "\n",
        "url =urllib.request.urlopen('https://www.linkedin.com/jobs/search?keywords=Data%20Analytics&location=United%20States&locationId=&geoId=103644278&f_TPR=&f_SB2=3&f_PP=102571732%2C104116203%2C102277331%2C103112676%2C102380872%2C106224388%2C102448103%2C103736294%2C103918656%2C103039849&f_WT=1%2C2&position=1&pageNum=0')\n",
        "driver.get(url)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IEpqCPdy0gGX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import uuid\n",
        "from os.path import exists\n",
        "from  openpyxl import load_workbook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2Ueiptm0gGX"
      },
      "outputs": [],
      "source": [
        "SCROLL_PAUSE_TIME = 0.5\n",
        "\n",
        "# Get scroll height\n",
        "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "i=0\n",
        "while i in range(0,10):\n",
        "    # Scroll down to bottom\n",
        "    i=i+1\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    # Wait to load page\n",
        "    time.sleep(SCROLL_PAUSE_TIME)\n",
        "# get a list of all the listings elements's in the side bar\n",
        "list_items = driver.find_elements_by_class_name(\"job-search-card\")\n",
        "print(len(list_items))\n",
        "while len(list_items) <= 1000: \n",
        "    seemore=  driver.find_elements_by_class_name(\"infinite-scroller__show-more-button\")\n",
        "    if seemore:\n",
        "        seemore[0].click()\n",
        "    # get a list of all the listings elements's in the side bar\n",
        "    list_items = driver.find_elements_by_class_name(\"job-search-card\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6EaemOR0gGZ",
        "outputId": "10dc541c-8556-4ca7-8f48-b231f1d46607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "print(len(list_items))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mEwUeuO0gGZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(len(list_items))\n",
        "# Empty dataframe\n",
        "dfJobs= pd.DataFrame(columns=['JobId','JobTitle','Source','Company','Description','PayRange','Skills','Location', 'HiringTimeLine', 'WhenPosted'])\n",
        "# scrolls a single page:\n",
        "i=0       \n",
        "pages=  driver.find_elements_by_class_name(\"jobs-search__results-list\")\n",
        "# for i in range(1,10):\n",
        "#     driver.execute_script(\"window.scrollTo(0, 1080);\")\n",
        "#     time.sleep(5)\n",
        "for job in list_items:\n",
        "    payrange='0'\n",
        "    i=i+1\n",
        "    try:\n",
        "        driver.execute_script(\"arguments[0].scrollIntoView();\", job)\n",
        "        id= job.get_attribute('class')        \n",
        "        job.click()  \n",
        "        time.sleep(3)\n",
        "    except:\n",
        "        driver.find_element_by_id(id)  \n",
        "        job.click()\n",
        "    payrangeel= driver.find_elements_by_class_name(\"compensation__salary\")\n",
        "    if len(payrangeel) >0:\n",
        "        payrange= payrangeel[0].text\n",
        "    jobtext= job.text.split('\\n')\n",
        "    if(len(job.text.split('\\n')) >=3):\n",
        "        position = jobtext[1]  \n",
        "        company = jobtext[2]  \n",
        "        location = jobtext[3]  \n",
        "        activehiring = jobtext[len(jobtext)-2]  \n",
        "        whenposted = jobtext[len(jobtext)-1]  \n",
        "    payrangeregex= r'^((?=.*[1-9]|0)(?:\\d{1,3}\\,\\d{1,3}\\$-\\d{1,3}\\,\\d{1,3}\\$))|((?=.*[1-9]|0)(?:\\$\\d{1,3}\\,\\d{1,3}\\-\\$\\d{1,3}\\,\\d{1,3}))|((?=.*[1-9]|0)(?:\\$\\d{1,3}\\,\\d{1,3}))|((?=.*[1-9]|0)(?:\\d{1,3}\\,\\d{1,3}\\$))'\n",
        "    benefitsel = driver.find_elements_by_class_name(\"featured-benefits__benefit-list\")         \n",
        "    details = driver.find_elements_by_class_name(\"show-more-less-html__markup\")    \n",
        "    for e in details:\n",
        "        detail =e.text      \n",
        "        items = e.find_elements_by_tag_name(\"li\")#             \n",
        "        for item in items:\n",
        "            text = item.get_attribute('innerText')\n",
        "            detail = detail + text\n",
        "        if(payrange == '0'):\n",
        "            p= re.compile(payrangeregex)\n",
        "            result= p.search(detail)\n",
        "            if result is not None:\n",
        "                payrange =result.group(0)\n",
        "        detail = re.sub(r'^.*?About Us', '', detail)\n",
        "        detail = re.sub(r'^.*?Responsibilities', '', detail)\n",
        "        detail = re.sub(r'\\n', '', detail)   \n",
        "    jobid=str(uuid.uuid1())\n",
        "    print(i)\n",
        "    dfJobs.loc[i] = [jobid,position,\"Linkedln\",company,detail,payrange, \"Python\",location,activehiring,whenposted]\n",
        "    #Skills\n",
        "    baselineskillscategory = ['statistics', 'machine learning', 'deep learning', 'r', 'python', 'nlp', 'data engineering', \n",
        "                      'business', 'software',  'cloud' , 'aws','sas' ]\n",
        "    baselineskillssubcat = ['regression','predictive modeling','clustering','time series', 'pca'] \n",
        "\n",
        "# write data to excel\n",
        "file_exists = exists('linkiedln_scraped_jobs.xlsx')\n",
        "print(len(dfJobs))\n",
        "if(file_exists):\n",
        "    print('out')\n",
        "    writer = pd.ExcelWriter('linkiedln_scraped_jobs.xlsx', engine='openpyxl')\n",
        "    # try to open an existing workbook\n",
        "    writer.book = load_workbook('linkiedln_scraped_jobs.xlsx')\n",
        "    # read existing file\n",
        "    reader = pd.read_excel(r'linkiedln_scraped_jobs.xlsx')\n",
        "    # write out the new sheet\n",
        "    dfJobs.to_excel(writer,index=False,header=True,startrow=len(reader)+1)\n",
        "    writer.save()\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kQv-naKo0gGb"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "  \n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "  \n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "#Custom fn\n",
        "def append_df_to_excel(df, excel_path):\n",
        "    df_excel = pd.read_excel(excel_path)\n",
        "    result = pd.concat([df_excel, df], ignore_index=True)\n",
        "    result.to_excel(excel_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PapmcQ6R0gGb",
        "outputId": "725ca80a-281b-4991-aee0-39f4870da743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('goal', 0.31175220012664795), ('that', 0.2805410623550415), ('equal', 0.262502521276474)]\n",
            "[('new', 0.573984682559967), ('to', 0.5652336478233337), ('you', 0.5584569573402405)]\n",
            "[('and', 0.9240734577178955), ('the', 0.9227239489555359), ('in', 0.9224685430526733)]\n",
            "[('and', 0.9680303931236267), ('on', 0.9670660495758057), ('with', 0.9658727645874023)]\n",
            "[('the', 0.9751505255699158), ('with', 0.9747650623321533), ('years', 0.9731864333152771)]\n",
            "[('to', 0.9791482090950012), ('&', 0.9788229465484619), ('.', 0.9783667922019958)]\n",
            "[(',', 0.9916662573814392), ('(', 0.9913524985313416), ('and', 0.9913517236709595)]\n",
            "[('and', 0.997016429901123), (',', 0.9966552257537842), ('in', 0.9965842962265015)]\n",
            "[(',', 0.9975932240486145), (')', 0.9970972537994385), ('(', 0.9969722628593445)]\n",
            "[('in', 0.9980028867721558), ('and', 0.9979947805404663), (',', 0.9979826807975769)]\n"
          ]
        }
      ],
      "source": [
        "# similarity check and prediction\n",
        "data=[]\n",
        "# iterate through each sentence in the file\n",
        "for k in dfJobs['Description']:\n",
        "    for i in sent_tokenize( k):\n",
        "        temp = []\n",
        "      \n",
        "    # tokenize the sentence into words\n",
        "        for j in word_tokenize(i):\n",
        "            temp.append(j.lower())\n",
        "  \n",
        "        data.append(temp)\n",
        "    # Create CBOW model\n",
        "    \n",
        "    model1 = gensim.models.Word2Vec(data, min_count = 3, vector_size = 100, window = 5)\n",
        "    \n",
        "\n",
        "    # Create Skip Gram model\n",
        "    model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100,window = 5, sg = 2)\n",
        "    #print(data)\n",
        "\n",
        "    print(model2.wv.similar_by_word(\"python\",topn=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ng9o0XhW0gGc"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.utils import simple_preprocess    \n",
        "\n",
        "def tidy_sentence(sentence, vocabulary):\n",
        "    return [word for word in simple_preprocess(sentence) if word in vocabulary]    \n",
        "\n",
        "def compute_sentence_similarity(sentence_1, sentence_2, model_wv):\n",
        "    vocabulary = set(model_wv.index2word)    \n",
        "    tokens_1 = tidy_sentence(sentence_1, vocabulary)    \n",
        "    tokens_2 = tidy_sentence(sentence_2, vocabulary)    \n",
        "    return model_wv.n_similarity(tokens_1, tokens_2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYOWWfkw0gGd",
        "outputId": "a0c4b973-8f2d-437a-a424-9204caa728e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "UUID('6cb10e1c-d21a-11ec-9e0d-bc117dcdeff0')"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import uuid \n",
        "uuid.uuid1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebSDP6yK0gGd",
        "outputId": "da3f28f1-2709-441a-b5f4-a34e15575ae9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'718fff2e-d21a-11ec-84a5-bc117dcdeff0'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "str(uuid.uuid1())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LUsQAvyR0gGe"
      },
      "outputs": [],
      "source": [
        "#Convert most freq words to dataframe for plotting bar plot\n",
        "top_words = get_top_n_words(lem_corpus)\n",
        "top_df = pd.DataFrame(top_words)\n",
        "top_df.columns=[\"Word\", \"Freq\"]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "NLP Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}